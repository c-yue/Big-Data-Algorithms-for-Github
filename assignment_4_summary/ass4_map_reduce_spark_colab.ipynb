{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfstpKhUM8x7"
      },
      "source": [
        "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Assignment 4 - MapReduce and Spark</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwndZSiM8x8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In this exercise you is asked to use Spark for implementing an algorithm that applies computations on documents and dataframes.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Execute the following cell in order to initialize Spark**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3shtJGpOM8x8"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar zxvf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#import of the SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#inizialization of the Spark Session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Assignment4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK49MFw9M8yA"
      },
      "source": [
        "# Analysing documents\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We have already seen that MapReduce procedures are good in analyzing text-files.\n",
        "    \n",
        "The provided data comes from a scraping operation on the website https://www.vagalume.com.br/ and is available on kaggle:\n",
        "    \n",
        "https://www.kaggle.com/neisse\n",
        "    \n",
        "\n",
        "    \n",
        "The assignment is divided in 2 parts:\n",
        "    \n",
        "* Part 1 is focused on MapReduce \n",
        "    \n",
        "* Part 2  is focuses on dataframes\n",
        "    </font>\n",
        "    </p>\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>Notice that  dataset is noisy and shows all the typical issues related with data coming from this procedure (duplicated entries, etc).</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1zsiET6ADyb"
      },
      "source": [
        "# Part 1 -  MapReduce\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In the provided folder you can find a set of documents/files containing  descriptions of songs (lyrics and additional informations). Specifically in each file:\n",
        "\n",
        "- the first line is the idiom/language\n",
        "- the second line is the title of a song\n",
        "- the third line is the relative url of the song of the original website\n",
        "- from fourth line on the text you find the lyrics of the song.\n",
        "    </font>\n",
        "    </p>\n",
        "\n",
        "## Exercise 1 - (2 points) - Song's lyrics \n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a Spark MapReduce procedure that reads the documents and checks how many song's lyrics appear at least two times.\n",
        "\n",
        "In the data-interpretation of this exercise you can consider that two files represent the same lyric if the url (3rd line of each file) is the same.\n",
        "\n",
        " </font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>Notice that  you can reuse any code that was made available for the previous labs/assignments or that you already developed in these contexts.</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font  size=\"5\" color='#91053d'>The dataset is big, some cells use sample set to calculate. But I'm confident on the code, especially Ex3.1 and Ex3.2. Please read the code in detail professor, merci!</font>**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mWzxx6LcZ4BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark import SparkContext"
      ],
      "metadata": {
        "id": "p6jveDIjG4tI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# link to file address\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPHzBTHgJM4m",
        "outputId": "d96ad6c5-c7f7-47f5-88e7-52679c9f66bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -uq '/content/gdrive/MyDrive/assignment_4_summary/lyrics_files_idioms.zip' -d '/content/gdrive/MyDrive/assignment_4_summary'"
      ],
      "metadata": {
        "id": "IyZ4W_wjI8MZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt-Asx5OADyc"
      },
      "outputs": [],
      "source": [
        "### Write here your code\n",
        "\n",
        "# prepare spark environment\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# read songs data\n",
        "file_path = r'/content/gdrive/MyDrive/assignment_4_summary/lyrics_files_idioms'\n",
        "file_list = os.listdir(file_path)\n",
        "\n",
        "# transfer each song to a list and integrate songs\n",
        "web_list = []\n",
        "lines_list = []\n",
        "for file in file_list:\n",
        "  new_path = '/'.join([file_path,file])\n",
        "  f = open(new_path)\n",
        "  lines = f.readlines()\n",
        "  web_list.append(lines[1])\n",
        "  lines_list.append(lines)\n",
        "\n",
        "# make rdd and map songs\n",
        "songs_rdd = sc.parallelize(lines_list)\n",
        "songs_map_rdd = songs_rdd.map(lambda x: (x[1],1))\n",
        "\n",
        "# count and filter to get frequent words (>=2)\n",
        "songs_reduce = songs_map_rdd.reduceByKey(lambda x,y: x+y)\n",
        "songs_2_times = songs_reduce.filter(lambda x: x[1] >= 2)\n",
        "\n",
        "# count number of frequent words\n",
        "songs_2_times.count()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERI3dTTEADyc"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "### 2.1 - (1 point) - Distinct songs\n",
        "Provide a Spark MapReduce procedure that provides how many distinct song's lyrics are present.\n",
        "\n",
        "Also in this case consider the uri as key: two files represent the same lyric if the url is equal.\n",
        "\n",
        "### 2.2 - (1 point) - Chaining\n",
        "According to your implementation of Exercise 1, can you chain MapReduce additional MapReduce steps for solving Exercise 2.2? \n",
        "\n",
        "Provide the code for 2.1 and anwer for 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48kXX5xfADyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f52c73b-f355-458b-8e2d-8b8f1ef42bb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12745"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "### Write here your code followed by the answer to question 2.2\n",
        "\n",
        "# use url to ignore duplicates\n",
        "web_list = []\n",
        "file_path_distinct = []\n",
        "lines_list = []\n",
        "for file in file_list:\n",
        "  new_path = '/'.join([file_path,file])\n",
        "  f = open(new_path)\n",
        "  lines = f.readlines()\n",
        "  # ignore duplicated songs\n",
        "  if lines[1] not in web_list:\n",
        "    web_list.append(lines[1])\n",
        "    file_path_distinct.append(new_path)\n",
        "    lines_list.append(lines)\n",
        "\n",
        "\n",
        "# make rdd and extract songs url\n",
        "songs_rdd = sc.parallelize(lines_list)\n",
        "songs_map_rdd = songs_rdd.map(lambda x: x[1])\n",
        "\n",
        "# count distinct songs\n",
        "songs_map_rdd.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QZR9_vADyd"
      },
      "source": [
        "# Exercise 3\n",
        "\n",
        "### 3.1 - (3 points) - Most common word for language\n",
        "\n",
        "Now that you discovered the duplicated documents consider just one occurence of each song's lyric and define a MapReduce procedure that finds the most common word for each language (of course you must remove stop words).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font  size=\"5\" color='#91053d'>The dataset is big, some cells use sample set to calculate. But I'm confident on the code, especially Ex3.1 and Ex3.2. Please read the code in detail professor, merci!</font>**"
      ],
      "metadata": {
        "id": "pjZUqKPUbVL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "AQx7Y8H4ADye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c902f6c-8cef-4328-db7f-38e5a6598f13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ENGLISH', ('I', 1430195)),\n",
              " ('SPANISH', ('que', 34703)),\n",
              " ('NA', ('oh', 21806)),\n",
              " ('KINYARWANDA', ('ni', 470)),\n",
              " ('GERMAN', ('Du', 622)),\n",
              " ('FRENCH', ('de', 598)),\n",
              " ('FINNISH', ('jotain', 64)),\n",
              " ('SWEDISH', ('ah', 1296)),\n",
              " ('ESTONIAN', ('welcome', 225)),\n",
              " ('INDONESIAN', ('girls', 144)),\n",
              " ('CATALAN', ('Rambla', 64)),\n",
              " ('PORTUGUESE', ('que', 225021)),\n",
              " ('ITALIAN', ('che', 2486)),\n",
              " ('HAITIAN_CREOLE', ('pa', 1089)),\n",
              " ('POLISH', ('czy', 121)),\n",
              " ('NORWEGIAN', ('han', 144))]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "### Write here your code\n",
        "\n",
        "# def func to split text to words\n",
        "def text_process(text):\n",
        "  import string\n",
        "  import re\n",
        "  import nltk \n",
        "  # remove punct\n",
        "  def remove_punct(text):\n",
        "      text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "      text = re.sub('[0-9]+', '', text)\n",
        "      return text\n",
        "  # token\n",
        "  def tokenization(text):\n",
        "      text = re.split('\\W+', text)\n",
        "      return text\n",
        "  # remove stop word\n",
        "  nltk.download('stopwords')\n",
        "  stopword = nltk.corpus.stopwords.words('english')\n",
        "  def remove_stopwords(text):\n",
        "      text = [word for word in text if word not in stopword]\n",
        "      return text\n",
        "\n",
        "  return remove_stopwords(tokenization(remove_punct(text)))\n",
        "\n",
        "# prepare def for count word in a song\n",
        "def count_words(word_list):\n",
        "  word_num_list = []\n",
        "  for word in word_list:\n",
        "    word_num = (word, word_list.count(word))\n",
        "    word_num_list.append(word_num)\n",
        "  return word_num_list\n",
        "\n",
        "# prepare def for count word of songs by language\n",
        "def word_num_gp(word_num_list):\n",
        "  # dict to sum word_num for words\n",
        "  word_num_gp_dict = {}\n",
        "  for word_num in word_num_list:\n",
        "    if word_num[0] in word_num_gp_dict.keys():\n",
        "      word_num_gp_dict[word_num[0]] += word_num[1]\n",
        "    else:\n",
        "      word_num_gp_dict[word_num[0]] = word_num[1]\n",
        "  # transfer dict to listd\n",
        "  word_num_gp = []\n",
        "  for word in word_num_gp_dict:\n",
        "    word_num = word_num_gp_dict[word]\n",
        "    word_num_tuple = (word, word_num)\n",
        "    word_num_gp.append(word_num_tuple)\n",
        "  # return gp list\n",
        "  return word_num_gp\n",
        "\n",
        "# prepare def for rank word_num tuple for a language\n",
        "def top_word_num(word_num_list):\n",
        "  sorted_l=sorted(word_num_list, key=lambda t:t[1], reverse=True)\n",
        "  top_word_num = sorted_l[0]\n",
        "  return top_word_num\n",
        "\n",
        "# songs words list by language (for each song)\n",
        "songs_words_by_lang = songs_rdd.map(lambda x: (''.join(str.split(x[0],'\\n')), count_words(text_process(x[3]))))\n",
        "\n",
        "# words_num tuple by language (for songs in one language)\n",
        "word_num_by_lang = songs_words_by_lang.reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], word_num_gp(x[1])))\n",
        "\n",
        "# sort most frequent word by language\n",
        "top_word_by_lang = word_num_by_lang.map(lambda x: (x[0], top_word_num(x[1])))\n",
        "top_word_by_lang.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EeGMBWOADye"
      },
      "source": [
        "### 3.2 - (3 points) - Most common end/start words\n",
        "\n",
        "Finally discover, for each language, the most common ending and starting word (of course, also in this case) you must remove stop words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "_yUUGcoaADye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55fa235-6620-429e-8ed8-e7b6c324008e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ENGLISH', ('I', 831)),\n",
              " ('SPANISH', ('No', 14)),\n",
              " ('NA', ('Instrumental', 141)),\n",
              " ('KINYARWANDA', ('Romanji', 2)),\n",
              " ('GERMAN', ('Ich', 3)),\n",
              " ('FRENCH', ('chorus', 4)),\n",
              " ('FINNISH', ('Kerran', 1)),\n",
              " ('SWEDISH', ('Ah', 1)),\n",
              " ('ESTONIAN', ('Im', 1)),\n",
              " ('INDONESIAN', ('Ya', 1)),\n",
              " ('CATALAN', ('Rambla', 1)),\n",
              " ('PORTUGUESE', ('Eu', 416)),\n",
              " ('ITALIAN', ('Il', 2)),\n",
              " ('HAITIAN_CREOLE', ('Men', 1)),\n",
              " ('POLISH', ('nie', 1)),\n",
              " ('NORWEGIAN', ('lyrics', 1))]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "### Write here your code\n",
        "\n",
        "# songs start-end words list by language (for each song)\n",
        "# here extract start-end word by get the first and last element of word list produced by text_process function (here use -2 to get element because -1 is '')\n",
        "songs_words_by_lang = songs_rdd.map(lambda x: (''.join(str.split(x[0],'\\n')), count_words([text_process(x[3])[0]]+[text_process(x[3])[-2]])))\n",
        "\n",
        "# words_num tuple by language (for songs in one language)\n",
        "word_num_by_lang = songs_words_by_lang.reduceByKey(lambda x,y: x+y).map(lambda x: (x[0], word_num_gp(x[1])))\n",
        "\n",
        "# sort most frequent word by language\n",
        "top_word_by_lang = word_num_by_lang.map(lambda x: (x[0], top_word_num(x[1])))\n",
        "top_word_by_lang.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-QgxsF1M8yB"
      },
      "source": [
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**DataFrames**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBkYMzzIM8yB",
        "outputId": "2f6a99b7-c42d-4401-9a7b-96628f3a0472"
      },
      "source": [
        "# Part 2 - Dataframes\n",
        "\n",
        "In this part you can use Pandas Dataframes or  Spark Dataframes.  I suggest to use a Spark Dataframe\n",
        "end exploit the Pandas functionalities as we have seen in the 2nd assignment. Download the two available datasets at the link:\n",
        "\n",
        "https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres\n",
        "\n",
        "You can find two .cvs files: \n",
        "\n",
        "* artists-data.csv\n",
        "\n",
        "* lyrics-data.csv\n",
        "\n",
        "\n",
        "# Import artist data.\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The artist data in the .csv file can be stored in a dataframe. \n",
        "    \n",
        "Each row of the .csv file describes an artist and the columns represent the following data:\n",
        "    \n",
        "* Artist - The artist's name\n",
        "* Popularity - Popularity score at the date of scrapping\n",
        "* ALink - The link to the artist's page\n",
        "* AGenre - Primary musical genre of the artist\n",
        "* AGenres - A list (pay attention to the format) of genres the artist fits in\n",
        "    \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "# Import song's lyrics data.\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "    \n",
        "Each row of the .csv file describes a lyric and the columns represent the following data:\n",
        "    \n",
        "* ALink - The link to the webpage of the artist\n",
        "* SLink - The link to the webpage of the song\n",
        "* Idiom - The idiom of the lyric\n",
        "* Lyric - The lyrics\n",
        "* SName - The name of the song\n",
        "\n",
        "    \n",
        "\n",
        "</font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQn1a3PqM80l"
      },
      "source": [
        "#  Exercise 4 - (3 points) - Artist's genre\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a program that finds the artists for which the genre is not specified.\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,isnan,when,count"
      ],
      "metadata": {
        "id": "Cu4DKfYFX3Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0liD2NOVM8yE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d8e647-0a30-4f3a-f31f-8e65d24a98b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[]\n",
            "[]\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "### Write here your code\n",
        "\n",
        "# link to file address\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "artists_path = '/content/gdrive/MyDrive/assignment_4_summary/archive/artists-data.csv'\n",
        "lyrics_path = '/content/gdrive/MyDrive/assignment_4_summary/archive/lyrics-data.csv'\n",
        "dfs_art = spark.read.csv(artists_path, header=True, inferSchema=True)\n",
        "dfs_lrc = spark.read.csv(lyrics_path, header=True, inferSchema=True)\n",
        "\n",
        "print(dfs_art.where(col('Genre').isNull()).collect())\n",
        "print(dfs_art.filter(isnan(dfs_art.Genre)).collect())\n",
        "print(dfs_art.where(col('Genre').isNull()).count())\n",
        "print(dfs_art.filter(isnan(dfs_art.Genre)).count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPiQX9XvADyf"
      },
      "source": [
        "#  Exercise 5 - (3 points) - Duplicates\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a program that removes the duplicates in the artists (also in this case the URL is the key).\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHI0XYmQADyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc7a2d9-7b59-4c3e-81e9-42256c6c6afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row number with duplicate:\n",
            "3242\n",
            "number of distinct artist:\n",
            "2940\n",
            "number of distinct url:\n",
            "2940\n",
            "row number after remove duplicate\n",
            "2940\n",
            "+----------------+-----+----------+------------------+-----+--------------------+\n",
            "|          Artist|Songs|Popularity|              Link|Genre|              Genres|\n",
            "+----------------+-----+----------+------------------+-----+--------------------+\n",
            "|        Bon Jovi|  359|      31.2|        /bon-jovi/| Rock|Hard Rock; Rock; ...|\n",
            "|Daniel y Enrique|   12|       0.0|/daniel-y-enrique/|  Pop|    Pop; World Music|\n",
            "|        Daughtry|   78|       1.4|        /daughtry/| Rock|Rock; Pop/Rock; R...|\n",
            "|      ForestSide|    7|       0.0|      /forestside/| Rock|Rock Alternativo;...|\n",
            "|  Mamilo Mandala|   10|       0.0|  /mamilo-mandala/| Rock|Classic Rock; Har...|\n",
            "+----------------+-----+----------+------------------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Write here your code\n",
        "\n",
        "# explore\n",
        "print('row number with duplicate:')\n",
        "print(dfs_art.count())\n",
        "\n",
        "print('number of distinct artist:')\n",
        "print(dfs_art.select('Artist').distinct().count())\n",
        "print('number of distinct url:')\n",
        "print(dfs_art.select('Link').distinct().count())\n",
        "\n",
        "# remove duplicate\n",
        "dfs_art_no_dplc = dfs_art.dropDuplicates(subset=[c for c in dfs_art.columns if c in ['Artist', 'Link']])\n",
        "\n",
        "print('row number after remove duplicate')\n",
        "print(dfs_art_no_dplc.count())\n",
        "dfs_art_no_dplc.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wupDL79mADyg"
      },
      "source": [
        "#  Exercise 6 - (4 points)\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Provide a program that using dataframe return the 100 most popular artists and the lyrics of their songs.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV4ycqRKADyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8481be2-e742-4842-fd3a-22489eed569a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+----------+--------------------+--------------------+--------------------+\n",
            "|     Artist|          Link|Popularity|               SName|               SLink|               Lyric|\n",
            "+-----------+--------------+----------+--------------------+--------------------+--------------------+\n",
            "|ExaltaSamba|/exalta-samba/|      13.0|Como Nunca Amei N...|/exalta-samba/com...|Nunca pensei que ...|\n",
            "|ExaltaSamba|/exalta-samba/|      13.0|           Separação|/exalta-samba/sep...|Melhor assim.... ...|\n",
            "|ExaltaSamba|/exalta-samba/|      13.0|           Telegrama|/exalta-samba/tel...|Ah, que saudade d...|\n",
            "|ExaltaSamba|/exalta-samba/|      13.0|      Livre Pra Voar|/exalta-samba/liv...|\"Quando a gente s...|\n",
            "|ExaltaSamba|/exalta-samba/|      13.0|              É Você|/exalta-samba/e-v...|É você.. Meu dese...|\n",
            "+-----------+--------------+----------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Write here your code\n",
        "# rank\n",
        "dfs_art_sorted = dfs_art_no_dplc.sort('Popularity', ascending=False)\n",
        "# top artist\n",
        "dfs_art_top100 = dfs_art_sorted.limit(100)\n",
        "# join artist and songs\n",
        "dfs_art_top100_songs = dfs_art_top100.join(dfs_lrc, dfs_art_top100.Link == dfs_lrc.ALink, \"left_outer\")\n",
        "# extract useful columns\n",
        "dfs_art_top100_songs = dfs_art_top100_songs.select('Artist','Link','Popularity','SName','SLink','Lyric')\n",
        "\n",
        "dfs_art_top100_songs.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DatBVcKUM8yn"
      },
      "source": [
        "# 2 - Bonus \n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Using the approach you prefer (just Dataframes, hybrid approach)  :\n",
        "    \n",
        "* the 10 most common words in the lyrics of each artist\n",
        "* the 10 most common words for each genre. For this question we can use the primary genre of the artist.\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3EIzkQFM8yo"
      },
      "outputs": [],
      "source": [
        "# Write here your code and the detailed description of the MapReduce algorithm.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "ass4_map_reduce_spark_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "name": "BE4-Spark.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}